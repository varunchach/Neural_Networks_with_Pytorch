{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f10d4b3"
      },
      "source": [
        "## 1. Introduction to PyTorch\n",
        "\n",
        "PyTorch is an open-source machine learning framework that accelerates the path from research prototyping to production deployment. It's known for its flexibility and ease of use, especially for deep learning tasks.\n",
        "\n",
        "At its core, PyTorch provides two main features:\n",
        "\n",
        "1.  **Tensor computation** (like NumPy) with strong GPU acceleration.\n",
        "2.  **Automatic differentiation** for building and training neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf7b7f1e"
      },
      "source": [
        "## 2. PyTorch Tensors\n",
        "\n",
        "Tensors are the fundamental data structure in PyTorch. They are similar to NumPy arrays but can run on GPUs for accelerated computing. Let's explore how to create and manipulate them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "925f6846"
      },
      "source": [
        "### Creating Tensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.array([1,2,3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVyFo-9rbAce",
        "outputId": "dd49aa34-6f83-4201-f9d9-cdbb372632cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e39e12c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79eb24b9-2c09-411f-def8-bf0905fac348"
      },
      "source": [
        "# From a Python list\n",
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)\n",
        "print(f\"Tensor from list:\\n{x_data}\")\n",
        "\n",
        "# From a NumPy array\n",
        "import numpy as np\n",
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)\n",
        "print(f\"\\nTensor from NumPy:\\n{x_np}\")\n",
        "\n",
        "# With random or constant values\n",
        "x_ones = torch.ones(3, 5) # All ones, shape (2,2)\n",
        "print(f\"\\nOnes Tensor:\\n{x_ones}\")\n",
        "\n",
        "x_rand = torch.rand(4, 2) # Random values, shape (2,2)\n",
        "print(f\"\\nRandom Tensor:\\n{x_rand}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor from list:\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "\n",
            "Tensor from NumPy:\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "\n",
            "Ones Tensor:\n",
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.]])\n",
            "\n",
            "Random Tensor:\n",
            "tensor([[0.8182, 0.3496],\n",
            "        [0.4742, 0.0993],\n",
            "        [0.5138, 0.5519],\n",
            "        [0.5365, 0.9828]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10e14cf7"
      },
      "source": [
        "### Tensor Attributes\n",
        "\n",
        "Each tensor has attributes like its shape, datatype, and the device it's stored on (CPU or GPU)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.rand(2,4)\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQFQpA32cJvm",
        "outputId": "b4c6d419-eda4-406c-e21e-0673c6bc1c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0658, 0.3159, 0.1946, 0.7230],\n",
            "        [0.7692, 0.2115, 0.8430, 0.2698]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f615da7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b29a5b4-cb16-4882-a746-ced24d1b2fad"
      },
      "source": [
        "#tensor = torch.rand(2,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([2, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d8258af"
      },
      "source": [
        "### Operations on Tensors\n",
        "\n",
        "Tensors support a wide range of operations, including arithmetic, slicing, joining, and reshaping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdf050b0"
      },
      "source": [
        "#### Arithmetic Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cae03f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d47d10e6-5f57-4c1c-aa27-d3259ffbff89"
      },
      "source": [
        "tensor = torch.ones(10, 4)\n",
        "print(f\"Original Tensor:\\n{tensor}\")\n",
        "\n",
        "# Element-wise multiplication\n",
        "print(f\"\\nTensor * 10:\\n{tensor * 10}\")\n",
        "\n",
        "# Matrix multiplication\n",
        "tensor_2 = torch.rand(4, 16)\n",
        "print(tensor_2)\n",
        "print(f\"\\nMatrix multiplication (tensor @ tensor_2):\\n{tensor @ tensor_2}\")\n",
        "\n",
        "# In-place operations (denoted by a `_` suffix)\n",
        "print(f\"\\nOriginal tensor before in-place addition:\\n{tensor}\")\n",
        "tensor.add_(5) # Adds 5 to every element in-place\n",
        "print(f\"Tensor after in-place add_:\\n{tensor}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tensor:\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "\n",
            "Tensor * 10:\n",
            "tensor([[10., 10., 10., 10.],\n",
            "        [10., 10., 10., 10.],\n",
            "        [10., 10., 10., 10.],\n",
            "        [10., 10., 10., 10.],\n",
            "        [10., 10., 10., 10.],\n",
            "        [10., 10., 10., 10.],\n",
            "        [10., 10., 10., 10.],\n",
            "        [10., 10., 10., 10.],\n",
            "        [10., 10., 10., 10.],\n",
            "        [10., 10., 10., 10.]])\n",
            "tensor([[0.1275, 0.2365, 0.1008, 0.2576, 0.4904, 0.1128, 0.4794, 0.5043, 0.8731,\n",
            "         0.1078, 0.2949, 0.4313, 0.9936, 0.5067, 0.7579, 0.8405],\n",
            "        [0.6679, 0.9721, 0.1432, 0.7426, 0.6462, 0.0576, 0.7308, 0.5206, 0.9982,\n",
            "         0.0134, 0.1730, 0.2119, 0.1440, 0.1389, 0.9895, 0.3460],\n",
            "        [0.7166, 0.6088, 0.3165, 0.5079, 0.5406, 0.8502, 0.9433, 0.7054, 0.1809,\n",
            "         0.7977, 0.3480, 0.0017, 0.9330, 0.3649, 0.5884, 0.0963],\n",
            "        [0.0613, 0.2610, 0.1340, 0.6746, 0.3110, 0.2899, 0.7939, 0.1419, 0.3440,\n",
            "         0.2736, 0.5924, 0.4959, 0.6112, 0.4352, 0.7333, 0.0960]])\n",
            "\n",
            "Matrix multiplication (tensor @ tensor_2):\n",
            "tensor([[1.5733, 2.0784, 0.6946, 2.1828, 1.9883, 1.3104, 2.9474, 1.8723, 2.3962,\n",
            "         1.1925, 1.4083, 1.1408, 2.6818, 1.4457, 3.0692, 1.3788],\n",
            "        [1.5733, 2.0784, 0.6946, 2.1828, 1.9883, 1.3104, 2.9474, 1.8723, 2.3962,\n",
            "         1.1925, 1.4083, 1.1408, 2.6818, 1.4457, 3.0692, 1.3788],\n",
            "        [1.5733, 2.0784, 0.6946, 2.1828, 1.9883, 1.3104, 2.9474, 1.8723, 2.3962,\n",
            "         1.1925, 1.4083, 1.1408, 2.6818, 1.4457, 3.0692, 1.3788],\n",
            "        [1.5733, 2.0784, 0.6946, 2.1828, 1.9883, 1.3104, 2.9474, 1.8723, 2.3962,\n",
            "         1.1925, 1.4083, 1.1408, 2.6818, 1.4457, 3.0692, 1.3788],\n",
            "        [1.5733, 2.0784, 0.6946, 2.1828, 1.9883, 1.3104, 2.9474, 1.8723, 2.3962,\n",
            "         1.1925, 1.4083, 1.1408, 2.6818, 1.4457, 3.0692, 1.3788],\n",
            "        [1.5733, 2.0784, 0.6946, 2.1828, 1.9883, 1.3104, 2.9474, 1.8723, 2.3962,\n",
            "         1.1925, 1.4083, 1.1408, 2.6818, 1.4457, 3.0692, 1.3788],\n",
            "        [1.5733, 2.0784, 0.6946, 2.1828, 1.9883, 1.3104, 2.9474, 1.8723, 2.3962,\n",
            "         1.1925, 1.4083, 1.1408, 2.6818, 1.4457, 3.0692, 1.3788],\n",
            "        [1.5733, 2.0784, 0.6946, 2.1828, 1.9883, 1.3104, 2.9474, 1.8723, 2.3962,\n",
            "         1.1925, 1.4083, 1.1408, 2.6818, 1.4457, 3.0692, 1.3788],\n",
            "        [1.5733, 2.0784, 0.6946, 2.1828, 1.9883, 1.3104, 2.9474, 1.8723, 2.3962,\n",
            "         1.1925, 1.4083, 1.1408, 2.6818, 1.4457, 3.0692, 1.3788],\n",
            "        [1.5733, 2.0784, 0.6946, 2.1828, 1.9883, 1.3104, 2.9474, 1.8723, 2.3962,\n",
            "         1.1925, 1.4083, 1.1408, 2.6818, 1.4457, 3.0692, 1.3788]])\n",
            "\n",
            "Original tensor before in-place addition:\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "Tensor after in-place add_:\n",
            "tensor([[6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a99b31d6"
      },
      "source": [
        "#### Slicing and Indexing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b01a3a8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e32137-efbc-4854-d918-2b9149377bb2"
      },
      "source": [
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print(f\"Original Tensor:\\n{tensor}\")\n",
        "\n",
        "print(f\"\\nFirst row: {tensor[0]}\")\n",
        "print(f\"Second column: {tensor[:, 1]}\")\n",
        "print(f\"Element at (0, 1): {tensor[0, 1]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tensor:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "\n",
            "First row: tensor([1, 2, 3])\n",
            "Second column: tensor([2, 5, 8])\n",
            "Element at (0, 1): 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d06c80a8"
      },
      "source": [
        "#### Reshaping Tensors\n",
        "\n",
        "`view` and `reshape` allow you to change the shape of a tensor without changing its data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc9a47b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa28742-66d7-470b-9375-055e5095b2ab"
      },
      "source": [
        "tensor = torch.zeros(4, 4)\n",
        "print(tensor)\n",
        "print(f\"Original Tensor shape: {tensor.shape}\")\n",
        "\n",
        "reshaped_tensor = tensor.view(16) # Flatten to a 1D tensor\n",
        "print (reshaped_tensor)\n",
        "print(f\"Reshaped Tensor (view) shape: {reshaped_tensor.shape}\")\n",
        "\n",
        "reshaped_tensor_2 = tensor.reshape(8, 2) # Reshape to (2, 8). -1 infers the dimension.\n",
        "print(reshaped_tensor_2)\n",
        "print(f\"Reshaped Tensor (reshape) shape: {reshaped_tensor_2.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "Original Tensor shape: torch.Size([4, 4])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Reshaped Tensor (view) shape: torch.Size([16])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n",
            "Reshaped Tensor (reshape) shape: torch.Size([8, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "696efae5"
      },
      "source": [
        "## 3. Automatic Differentiation with `autograd`\n",
        "\n",
        "One of PyTorch's most powerful features is `autograd`, which automatically computes gradients for all operations on tensors that have `requires_grad=True`. This is crucial for training neural networks through backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cfb94c0"
      },
      "source": [
        "### Basic `autograd` Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f6f71aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c3e1014-a44e-42f9-c783-23b50cc26592"
      },
      "source": [
        "# Create a tensor with requires_grad=True to track computations\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Define a simple function\n",
        "y = x**2 + 3*x + 1\n",
        "\n",
        "\n",
        "# Compute gradients (backpropagation)\n",
        "y.backward()\n",
        "\n",
        "# Print the gradient of y with respect to x\n",
        "# Mathematically, dy/dx for y = x^2 + 3x + 1 is 2x + 3.\n",
        "# At x=2, dy/dx = 2*2 + 3 = 7.\n",
        "print(f\"Gradient of y with respect to x at x={x.item()}: {x.grad}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of y with respect to x at x=2.0: 7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd687595"
      },
      "source": [
        "### Gradients for Multiple Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2db3642",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c090aad-fc66-40c1-bcf2-3fc6ac61388e"
      },
      "source": [
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "z = x**2 + y**3 # z = x^2 + y^3\n",
        "\n",
        "z.backward() # Computes dz/dx and dz/dy\n",
        "\n",
        "# dz/dx = 2x. At x=1, dz/dx = 2.\n",
        "# dz/dy = 3y^2. At y=2, dz/dy = 3 * (2^2) = 12.\n",
        "print(f\"Gradient of z with respect to x at x={x.item()}: {x.grad}\")\n",
        "print(f\"Gradient of z with respect to y at y={y.item()}: {y.grad}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of z with respect to x at x=1.0: 2.0\n",
            "Gradient of z with respect to y at y=2.0: 12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4f9cfce"
      },
      "source": [
        "### Mathematical Derivation of Gradients\n",
        "\n",
        "The function defined was: $$z = x^2 + y^3$$\n",
        "\n",
        "To find the gradients of `z` with respect to `x` and `y`, we need to compute the partial derivatives:\n",
        "\n",
        "1.  **Partial derivative of `z` with respect to `x` (∂z/∂x)**:\n",
        "\n",
        "    $$\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}(x^2 + y^3)$$\n",
        "\n",
        "    Since `y` is treated as a constant when differentiating with respect to `x`:\n",
        "\n",
        "    $$\\frac{\\partial z}{\\partial x} = 2x + 0 = 2x$$\n",
        "\n",
        "    At the given value $x = 1.0$:\n",
        "\n",
        "    $$\\frac{\\partial z}{\\partial x} = 2 \\times 1.0 = 2.0$$\n",
        "\n",
        "    This matches the `x.grad` value of `2.0`.\n",
        "\n",
        "2.  **Partial derivative of `z` with respect to `y` (∂z/∂y)**:\n",
        "\n",
        "    $$\\frac{\\partial z}{\\partial y} = \\frac{\\partial}{\\partial y}(x^2 + y^3)$$\n",
        "\n",
        "    Since `x` is treated as a constant when differentiating with respect to `y`:\n",
        "\n",
        "    $$\\frac{\\partial z}{\\partial y} = 0 + 3y^2 = 3y^2$$\n",
        "\n",
        "    At the given value $y = 2.0$:\n",
        "\n",
        "    $$\\frac{\\partial z}{\\partial y} = 3 \\times (2.0)^2 = 3 \\times 4.0 = 12.0$$\n",
        "\n",
        "    This matches the `y.grad` value of `12.0`.\n",
        "\n",
        "PyTorch's `autograd` performs these symbolic and numerical computations automatically when `z.backward()` is called, providing the correct gradients without manual calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13475285"
      },
      "source": [
        "### Freezing Parameters\n",
        "\n",
        "Sometimes, you might want to freeze part of your model, meaning you don't want to compute gradients for certain parameters during training. This can be done by setting `requires_grad=False`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfae6dd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cdcca56-7208-4f49-a72c-e452ed554ecd"
      },
      "source": [
        "a = torch.ones(2, 2, requires_grad=True)\n",
        "b = torch.zeros(2, 2, requires_grad=True)\n",
        "print(a,b)\n",
        "# Detach b from the computation graph, so its gradients won't be computed\n",
        "c = a + b.detach()\n",
        "\n",
        "d = c * 2\n",
        "print(d)\n",
        "d.sum().backward() # Sum to get a scalar for backward\n",
        "\n",
        "print(f\"Gradient of a: {a.grad}\")\n",
        "print(f\"Gradient of b: {b.grad}\") # b.grad will be None as it was detached"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True) tensor([[0., 0.],\n",
            "        [0., 0.]], requires_grad=True)\n",
            "tensor([[2., 2.],\n",
            "        [2., 2.]], grad_fn=<MulBackward0>)\n",
            "Gradient of a: tensor([[2., 2.],\n",
            "        [2., 2.]])\n",
            "Gradient of b: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "799ffb9f"
      },
      "source": [
        "### Explanation of Freezing Parameters with `detach()`\n",
        "\n",
        "This example demonstrates how to control which tensors contribute to gradient computations by using the `detach()` method, effectively 'freezing' certain parts of the computation graph.\n",
        "\n",
        "Let's analyze the steps:\n",
        "\n",
        "1.  **`a = torch.ones(2, 2, requires_grad=True)`**\n",
        "    *   A tensor `a` of ones is created. `requires_grad=True` ensures that PyTorch will track operations on `a` to compute its gradients later.\n",
        "\n",
        "2.  **`b = torch.zeros(2, 2, requires_grad=True)`**\n",
        "    *   A tensor `b` of zeros is created, also with `requires_grad=True` initially. This means that, by default, its operations would also be tracked for gradients.\n",
        "\n",
        "3.  **`c = a + b.detach()`**\n",
        "    *   This is the crucial step. `b.detach()` creates a *new tensor* that shares the same data with `b` but is completely **removed from the current computation graph**. This means that when backpropagation occurs, no gradients will flow back to the original `b` tensor through this operation.\n",
        "    *   `c` is then computed by adding `a` and the detached version of `b`. Since `a` still has `requires_grad=True` and `b.detach()` does not, the resulting tensor `c` will have `requires_grad=True` because its computation depends on `a`.\n",
        "\n",
        "4.  **`d = c * 2`**\n",
        "    *   `d` is computed by multiplying `c` by 2. Since `c` has `requires_grad=True`, `d` also inherits this property.\n",
        "\n",
        "5.  **`d.sum().backward()`**\n",
        "    *   To perform backpropagation, `d` (a 2x2 tensor) is summed to create a scalar value. Then, `.backward()` is called on this scalar sum. This command triggers the calculation of gradients for all tensors in the computation graph that led to `d.sum()` and have `requires_grad=True`.\n",
        "\n",
        "### Gradient Results\n",
        "\n",
        "*   **`Gradient of a: {a.grad}`**\n",
        "    *   The gradient for `a` is calculated. Let's trace it:\n",
        "        *   `d = 2 * c`\n",
        "        *   `c = a + b_detached`\n",
        "        *   So, `d = 2 * (a + b_detached)`\n",
        "        *   The derivative of `d` with respect to `a` (ignoring `b_detached` as it's a constant in this path) is `∂d/∂a = 2`. Since `d` is summed (`d.sum()`) for `backward()`, the gradient for each element of `a` will be `2.0`.\n",
        "    *   Output: `tensor([[2., 2.], [2., 2.]])`\n",
        "\n",
        "*   **`Gradient of b: {b.grad}`**\n",
        "    *   The gradient for `b` is `None`. This is because `b` was explicitly `detach()`ed from the computation graph before it was used to compute `c`. Therefore, no path exists in the graph for gradients to flow back to `b` from `c` (and subsequently from `d` or `d.sum()`).\n",
        "\n",
        "This technique is useful when you want to train only specific parts of a model, or when you are using pre-trained features and don't want to update the weights of the feature extractor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0cf0928"
      },
      "source": [
        "# Task\n",
        "Enhance the PyTorch demo by adding sections on defining neural networks using `torch.nn`, introducing and implementing loss functions, explaining and instantiating optimizers, and outlining and implementing a basic neural network training loop using dummy data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2016fcf"
      },
      "source": [
        "## Introduction to Neural Networks\n",
        "\n",
        "### Subtask:\n",
        "Add a text cell introducing PyTorch's `torch.nn` module and its role in building neural networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32254cf6"
      },
      "source": [
        "## 4. Building Neural Networks with `torch.nn`\n",
        "\n",
        "PyTorch's `torch.nn` module is a fundamental component for building and training neural networks. It provides all the necessary building blocks for defining neural network architectures, handling parameters, and performing computations. Key components of `torch.nn` include:\n",
        "\n",
        "*   **`nn.Module`**: This is the base class for all neural network modules. Any custom neural network architecture or layer should inherit from `nn.Module`. It helps manage parameters, submodules, and provides methods like `forward()` for defining the computation flow.\n",
        "*   **Layers**: `torch.nn` offers a rich library of pre-built layers like `nn.Linear` (for fully connected layers), `nn.Conv2d` (for convolutional layers), `nn.MaxPool2d` (for pooling layers), `nn.ReLU` (for activation functions), and many more.\n",
        "*   **Activation Functions**: Various non-linear activation functions (e.g., ReLU, Sigmoid, Tanh) are available in `torch.nn.functional` or as `nn.Module` subclasses.\n",
        "*   **Loss Functions**: `torch.nn` also provides common loss functions (e.g., `nn.CrossEntropyLoss` for classification, `nn.MSELoss` for regression) to calculate the error between predicted and target values during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a9b08b9"
      },
      "source": [
        "## Define a Simple Neural Network\n",
        "\n",
        "### Subtask:\n",
        "Add a code cell to define a simple neural network using `torch.nn.Module`, including linear layers and an activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65555e6a",
        "outputId": "6d1547b9-0c19-4823-d83a-df6f052cb018"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# 1. Define a Python class named SimpleNeuralNetwork that inherits from torch.nn.Module.\n",
        "class SimpleNeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 2. Initialize two linear layers: self.linear_relu_stack as torch.nn.Sequential\n",
        "        # containing a torch.nn.Linear layer mapping from 20 input features to 50 hidden features,\n",
        "        # a torch.nn.ReLU activation, and another torch.nn.Linear layer mapping from 50 hidden features to 1 output feature.\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(20, 50), # 20 input features to 50 hidden features\n",
        "            nn.ReLU(),         # ReLU activation function\n",
        "            nn.Linear(50, 1)   # 50 hidden features to 1 output feature\n",
        "        )\n",
        "\n",
        "    # 3. Implement the forward method which takes an input x and passes it through self.linear_relu_stack.\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "# 4. Instantiate an object of SimpleNeuralNetwork and name it model.\n",
        "model = SimpleNeuralNetwork()\n",
        "\n",
        "# 5. Print the model object to display its architecture.\n",
        "print(\"Simple Neural Network Model Architecture:\")\n",
        "print(model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple Neural Network Model Architecture:\n",
            "SimpleNeuralNetwork(\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=20, out_features=50, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=50, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b451bf35"
      },
      "source": [
        "## Introduce Loss Functions\n",
        "\n",
        "### Subtask:\n",
        "Add a text cell to explain the concept of loss functions and introduce common PyTorch loss functions like `MSELoss` or `CrossEntropyLoss`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2129e293"
      },
      "source": [
        "### Loss Functions\n",
        "\n",
        "Loss functions (also known as cost functions or objective functions) are crucial components in training neural networks. They quantify the difference between the predicted output of the network and the actual target values. The goal of training a neural network is to minimize this loss, guiding the model to make more accurate predictions.\n",
        "\n",
        "During backpropagation, the gradients of the loss function with respect to the model's parameters are calculated. These gradients are then used by optimizers to adjust the parameters, iteratively improving the model's performance.\n",
        "\n",
        "PyTorch's `torch.nn` module provides various common loss functions:\n",
        "\n",
        "*   **`torch.nn.MSELoss` (Mean Squared Error Loss)**: This loss function is typically used for regression tasks, where the goal is to predict a continuous value. It calculates the mean of the squared differences between predicted and true values. Mathematically, it's defined as `(y_pred - y_true)^2`.\n",
        "\n",
        "*   **`torch.nn.CrossEntropyLoss`**: This is a widely used loss function for classification tasks, especially when dealing with multi-class classification. It combines `nn.LogSoftmax` and `nn.NLLLoss` in one single class. It measures the performance of a classification model whose output is a probability value between 0 and 1. Lower values mean better model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5942234e"
      },
      "source": [
        "## Implement a Loss Function\n",
        "\n",
        "### Subtask:\n",
        "Add a code cell demonstrating how to instantiate and use a loss function with example model outputs and target values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "021f78a8",
        "outputId": "f7723603-4fb4-467b-dfad-34662eec6531"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# 1. Instantiate a loss function (e.g., Mean Squared Error for regression)\n",
        "loss_fn = nn.MSELoss() # Or nn.CrossEntropyLoss() for classification\n",
        "print(f\"Instantiated Loss Function: {loss_fn}\\n\")\n",
        "\n",
        "# 2. Create example model outputs (predictions) and target values\n",
        "predictions = torch.randn(10, 1) # Example: 10 predictions, 1 output feature each\n",
        "targets = torch.randn(10, 1)    # Example: 10 target values, 1 output feature each\n",
        "print(targets)\n",
        "print(f\"Example Predictions:\\n{predictions.T}\\n\")\n",
        "print(f\"Example Targets:\\n{targets.T}\\n\")\n",
        "\n",
        "# 3. Calculate the loss\n",
        "loss = loss_fn(predictions, targets)\n",
        "\n",
        "# 4. Print the calculated loss\n",
        "print(f\"Calculated Loss (MSE): {loss.item()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instantiated Loss Function: MSELoss()\n",
            "\n",
            "tensor([[ 1.4518e+00],\n",
            "        [ 3.2571e-01],\n",
            "        [-2.0965e+00],\n",
            "        [ 4.2007e-01],\n",
            "        [ 1.2847e+00],\n",
            "        [ 1.7305e+00],\n",
            "        [-4.7806e-01],\n",
            "        [ 1.6695e-03],\n",
            "        [-6.6902e-02],\n",
            "        [ 2.2826e-01]])\n",
            "Example Predictions:\n",
            "tensor([[-1.8860,  0.0164, -0.6540, -0.0549,  0.9582,  1.8385,  1.8897, -0.3213,\n",
            "          0.2447,  0.7658]])\n",
            "\n",
            "Example Targets:\n",
            "tensor([[ 1.4518e+00,  3.2571e-01, -2.0965e+00,  4.2007e-01,  1.2847e+00,\n",
            "          1.7305e+00, -4.7806e-01,  1.6695e-03, -6.6902e-02,  2.2826e-01]])\n",
            "\n",
            "Calculated Loss (MSE): 1.9758189916610718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3bc1c15"
      },
      "source": [
        "## Introduce Optimizers\n",
        "\n",
        "### Subtask:\n",
        "Add a text cell to explain the purpose of optimizers in updating model parameters during training and mention common PyTorch optimizers like `SGD` or `Adam`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b71e154"
      },
      "source": [
        "### Optimizers\n",
        "\n",
        "Optimizers are algorithms or methods used to adjust the weights and biases (parameters) of a neural network during training. Their primary purpose is to minimize the loss function, thereby improving the model's performance and accuracy.\n",
        "\n",
        "**Role in Training:**\n",
        "After the network's output is compared to the true labels by a loss function, `autograd` computes the gradients of this loss with respect to each model parameter. The optimizer then takes these gradients and uses a specific algorithm (e.g., gradient descent, Adam, etc.) to update the parameters. This iterative process of forward pass, loss calculation, backpropagation (gradient computation), and parameter update (by the optimizer) is what drives the learning in a neural network.\n",
        "\n",
        "**Common PyTorch Optimizers:**\n",
        "PyTorch's `torch.optim` module provides various optimization algorithms:\n",
        "\n",
        "*   **`torch.optim.SGD` (Stochastic Gradient Descent)**: This is a fundamental optimizer that updates parameters in the direction opposite to the gradient of the loss function. While simple, it can be effective. \"Stochastic\" refers to the fact that it computes gradients and updates parameters based on small random subsets of the data (batches) rather than the entire dataset, which speeds up training.\n",
        "\n",
        "*   **`torch.optim.Adam` (Adaptive Moment Estimation)**: Adam is an adaptive learning rate optimization algorithm that's become very popular due to its efficiency and good performance in practice. It computes adaptive learning rates for each parameter by maintaining an exponentially decaying average of past gradients (momentum) and past squared gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27d842ff"
      },
      "source": [
        "## Instantiate an Optimizer\n",
        "\n",
        "### Subtask:\n",
        "Add a code cell to instantiate an optimizer, linking it to the parameters of the previously defined simple neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3c9a3c1",
        "outputId": "873fc164-706b-4cb8-8ad9-30645e826bf5"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Instantiate an optimizer (e.g., Adam)\n",
        "# We link the optimizer to the parameters of our 'model' (SimpleNeuralNetwork) defined earlier.\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(f\"Instantiated Optimizer: {optimizer}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instantiated Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f71d53e"
      },
      "source": [
        "## Outline a Basic Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Add a text cell describing the key steps involved in a neural network training loop: forward pass, loss calculation, backward pass, and optimizer step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aa9b8e0"
      },
      "source": [
        "### Training Loop Outline\n",
        "\n",
        "A neural network training loop is the iterative process where a model learns from data. Each iteration (or *epoch* for a full pass over the dataset) typically involves the following core steps:\n",
        "\n",
        "1.  **Forward Pass**: The model takes the input data and processes it through its layers to generate an output, which are the model's predictions. For example, if we have input `x` and our model is `model`, this step would look like `predictions = model(x)`.\n",
        "\n",
        "2.  **Loss Calculation**: The generated predictions are compared against the actual target values (ground truth). A loss function quantifies the discrepancy or error between the predictions and the targets. The result is a single scalar value representing the model's performance on the current batch of data. For example, `loss = loss_fn(predictions, targets)`.\n",
        "\n",
        "3.  **Backward Pass (Backpropagation)**: This is where `autograd` comes into play. After calculating the loss, `loss.backward()` is called. This computes the gradients of the loss with respect to all parameters that have `requires_grad=True`. These gradients indicate how much each parameter contributed to the error and in what direction it needs to be adjusted to reduce the loss.\n",
        "\n",
        "4.  **Optimizer Step**: The optimizer uses the calculated gradients to update the model's parameters. Before performing the update, `optimizer.zero_grad()` is typically called to clear any previously accumulated gradients (as PyTorch accumulates gradients by default). Then, `optimizer.step()` applies the updates to the parameters based on the chosen optimization algorithm (e.g., SGD, Adam) and the learning rate, moving the parameters in the direction that minimizes the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f266004"
      },
      "source": [
        "## Implement a Simple Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Implement a minimal training loop using dummy data, integrating the defined network, loss function, and optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Rows: 1000:\n",
        "\n",
        "Batch : 100\n",
        "batch 1 : 100\n",
        "Batch 2: 100\n",
        "Batch 3: 100\n",
        ".\n",
        ".\n"
      ],
      "metadata": {
        "id": "UvLgIp21h5T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a3aaa78",
        "outputId": "29504871-ed6d-4954-c4ac-813ed8c4b97a"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 1. Define a train_loop function\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train() # Set the model to training mode\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # a. Forward pass\n",
        "        pred = model(X)\n",
        "\n",
        "        # b. Loss calculation\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # c. Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # d. Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # e. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            current = batch * len(X)\n",
        "            print(f\"loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "# 4. Create dummy input data X and target data y\n",
        "X_dummy = torch.randn(1000, 20) # 1000 samples, 20 features (matching model input)\n",
        "y_dummy = torch.randn(1000, 1)  # 1000 samples, 1 target (matching model output)\n",
        "\n",
        "# 5. Create a TensorDataset and then a DataLoader\n",
        "dataset = TensorDataset(X_dummy, y_dummy)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 6. Set the number of training epochs\n",
        "epochs = 10\n",
        "print(f\"\\nStarting training for {epochs} epochs...\")\n",
        "\n",
        "# Instantiate model, loss_fn, and optimizer (if not already done in the current scope)\n",
        "# This assumes 'model', 'loss_fn', and 'optimizer' from previous cells are available\n",
        "# If running this cell independently, uncomment and run the following:\n",
        "# from torch import nn\n",
        "# import torch.optim as optim\n",
        "# class SimpleNeuralNetwork(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.linear_relu_stack = nn.Sequential(\n",
        "#             nn.Linear(20, 50),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(50, 1)\n",
        "#         )\n",
        "#     def forward(self, x):\n",
        "#         return self.linear_relu_stack(x)\n",
        "# model = SimpleNeuralNetwork()\n",
        "# loss_fn = nn.MSELoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 7. Run the training loop for the specified number of epochs\n",
        "for t in range(epochs):\n",
        "    print(f\"\\nEpoch {t+1}\\n----------------------------------\")\n",
        "    epoch_loss = train_loop(dataloader, model, loss_fn, optimizer)\n",
        "    print(f\"Epoch {t+1} finished! Average Loss: {epoch_loss:>8f}\")\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "Epoch 1\n",
            "----------------------------------\n",
            "loss: 0.924623  [    0/ 1000]\n",
            "Epoch 1 finished! Average Loss: 1.049480\n",
            "\n",
            "Epoch 2\n",
            "----------------------------------\n",
            "loss: 1.198885  [    0/ 1000]\n",
            "Epoch 2 finished! Average Loss: 1.007161\n",
            "\n",
            "Epoch 3\n",
            "----------------------------------\n",
            "loss: 1.217597  [    0/ 1000]\n",
            "Epoch 3 finished! Average Loss: 0.996514\n",
            "\n",
            "Epoch 4\n",
            "----------------------------------\n",
            "loss: 1.231169  [    0/ 1000]\n",
            "Epoch 4 finished! Average Loss: 0.999027\n",
            "\n",
            "Epoch 5\n",
            "----------------------------------\n",
            "loss: 1.020290  [    0/ 1000]\n",
            "Epoch 5 finished! Average Loss: 0.981559\n",
            "\n",
            "Epoch 6\n",
            "----------------------------------\n",
            "loss: 0.952016  [    0/ 1000]\n",
            "Epoch 6 finished! Average Loss: 0.965705\n",
            "\n",
            "Epoch 7\n",
            "----------------------------------\n",
            "loss: 0.768926  [    0/ 1000]\n",
            "Epoch 7 finished! Average Loss: 0.985494\n",
            "\n",
            "Epoch 8\n",
            "----------------------------------\n",
            "loss: 1.109284  [    0/ 1000]\n",
            "Epoch 8 finished! Average Loss: 0.962578\n",
            "\n",
            "Epoch 9\n",
            "----------------------------------\n",
            "loss: 0.708580  [    0/ 1000]\n",
            "Epoch 9 finished! Average Loss: 0.947465\n",
            "\n",
            "Epoch 10\n",
            "----------------------------------\n",
            "loss: 0.967164  [    0/ 1000]\n",
            "Epoch 10 finished! Average Loss: 0.948564\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ae285ba"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the additions made to the notebook, completing the PyTorch demo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98da97d8"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `torch.nn` module was introduced, highlighting its role in building neural networks through `nn.Module` for architecture definition, various layers (e.g., `nn.Linear`, `nn.Conv2d`), activation functions, and loss functions.\n",
        "*   A `SimpleNeuralNetwork` class inheriting from `torch.nn.Module` was defined with two linear layers (20 input features to 50 hidden features, and 50 hidden features to 1 output feature) and a `nn.ReLU` activation function. An instance of this model was successfully created.\n",
        "*   Loss functions were explained as quantifying the difference between predicted and actual values, with `torch.nn.MSELoss` (for regression) and `torch.nn.CrossEntropyLoss` (for classification) introduced as common examples.\n",
        "*   An `nn.MSELoss` function was instantiated and demonstrated calculating a loss of approximately 1.5086 using dummy predictions and target values of shape (10, 1).\n",
        "*   Optimizers were detailed as algorithms for adjusting neural network parameters to minimize loss, with `torch.optim.SGD` and `torch.optim.Adam` presented as key PyTorch examples.\n",
        "*   An `Adam` optimizer was successfully instantiated with a learning rate of 0.001, linked to the parameters of the previously defined `SimpleNeuralNetwork` model.\n",
        "*   The four core steps of a neural network training loop were outlined: Forward Pass, Loss Calculation, Backward Pass (backpropagation), and Optimizer Step (including `optimizer.zero_grad()` and `optimizer.step()`).\n",
        "*   A functional training loop was implemented using dummy data (1000 samples, 20 input features, 1 output target). Over 10 epochs, the average loss decreased from approximately 1.049 in Epoch 1 to around 0.948 in Epoch 10, indicating successful integration and basic learning.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The PyTorch demo now comprehensively covers the fundamental building blocks for neural network development, from model definition to training. This provides a solid educational foundation.\n",
        "*   To further enhance the demo, the next logical step would be to introduce actual datasets (e.g., from `torchvision.datasets`) and demonstrate the training loop on a more realistic problem, potentially including evaluation metrics and a separate validation set.\n"
      ]
    }
  ]
}